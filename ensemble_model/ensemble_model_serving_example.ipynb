{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Inference with Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "* [1. Set up client](#client)\n",
    "* [2. Set up a model repository](#setup_model)\n",
    "* [3. Set up the ensemble model](#setup_ensemble)\n",
    "* [4. Set up the ensemble scheduler](#setup_scheduler)\n",
    "* [5. Run triton inference serever](#run_server)\n",
    "* [6. Request image classification](#request)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='client'></a>\n",
    "### 1.Set up client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Build docker image for client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "git clone https://github.com/NVIDIA/triton-inference-server\n",
    "cd triton-inference-server\n",
    "git checkout r20.03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to modify the dockerfile due to this issue (https://github.com/NVIDIA/triton-inference-server/issues/1453)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-inference-server/Dockerfile.client\n",
    "# Copyright (c) 2019-2020, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions\n",
    "# are met:\n",
    "#  * Redistributions of source code must retain the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer.\n",
    "#  * Redistributions in binary form must reproduce the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer in the\n",
    "#    documentation and/or other materials provided with the distribution.\n",
    "#  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived\n",
    "#    from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
    "# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
    "# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
    "# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
    "# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
    "# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
    "# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
    "# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "# Default setting is building on nvidia/cuda:10.1-devel-ubuntu18.04\n",
    "ARG BASE_IMAGE=nvidia/cuda:10.1-devel-ubuntu18.04\n",
    "\n",
    "FROM ${BASE_IMAGE}\n",
    "\n",
    "# Ensure apt-get won't prompt for selecting options\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "            software-properties-common \\\n",
    "            autoconf \\\n",
    "            automake \\\n",
    "            build-essential \\\n",
    "            cmake \\\n",
    "            curl \\\n",
    "            git \\\n",
    "            libopencv-dev \\\n",
    "            libopencv-core-dev \\\n",
    "            libssl-dev \\\n",
    "            libtool \\\n",
    "            pkg-config \\\n",
    "            python3 \\\n",
    "            python3-pip \\\n",
    "            python3-dev \\\n",
    "            rapidjson-dev && \\\n",
    "    pip3 install --upgrade wheel setuptools && \\\n",
    "    pip3 install --upgrade grpcio-tools\n",
    "\n",
    "# Build expects \"python\" executable (not python3).\n",
    "RUN rm -f /usr/bin/python && \\\n",
    "    ln -s /usr/bin/python3 /usr/bin/python\n",
    "\n",
    "# Build the client library and examples\n",
    "WORKDIR /workspace\n",
    "COPY VERSION .\n",
    "COPY build build\n",
    "COPY src/clients src/clients\n",
    "COPY src/core src/core\n",
    "\n",
    "RUN cd build && \\\n",
    "    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX:PATH=/workspace/install && \\\n",
    "    make -j16 trtis-clients\n",
    "RUN cd install && \\\n",
    "    export VERSION=`cat /workspace/VERSION` && \\\n",
    "    tar zcf /workspace/v$VERSION.clients.tar.gz *\n",
    "\n",
    "# For CI testing need to install a test script.\n",
    "COPY qa/L0_client_tar/test.sh /tmp/test.sh\n",
    "\n",
    "# Install an image needed by the quickstart and other documentation.\n",
    "COPY qa/images/mug.jpg images/mug.jpg\n",
    "\n",
    "# Install the dependencies needed to run the client examples. These\n",
    "# are not needed for building but including them allows this image to\n",
    "# be used to run the client examples.\n",
    "RUN pip3 install --upgrade install/python/tensorrtserver-*.whl numpy pillow\n",
    "\n",
    "ENV PATH //workspace/install/bin:${PATH}\n",
    "ENV LD_LIBRARY_PATH /workspace/install/lib:${LD_LIBRARY_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd triton-inference-server\n",
    "docker build -t tritonserver_client -f Dockerfile.client ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implement client for ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ensemble_image_client.py\n",
    "#!/usr/bin/env python\n",
    "# Copyright (c) 2018-2020, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions\n",
    "# are met:\n",
    "#  * Redistributions of source code must retain the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer.\n",
    "#  * Redistributions in binary form must reproduce the above copyright\n",
    "#    notice, this list of conditions and the following disclaimer in the\n",
    "#    documentation and/or other materials provided with the distribution.\n",
    "#  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
    "#    contributors may be used to endorse or promote products derived\n",
    "#    from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
    "# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
    "# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
    "# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
    "# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
    "# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
    "# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
    "# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from builtins import range\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from tensorrtserver.api import *\n",
    "import tensorrtserver.api.model_config_pb2 as model_config\n",
    "\n",
    "if sys.version_info >= (3, 0):\n",
    "  import queue\n",
    "else:\n",
    "  import Queue as queue\n",
    "\n",
    "class UserData:\n",
    "    def __init__(self):\n",
    "        self._completed_requests = queue.Queue()\n",
    "\n",
    "# Callback function used for async_run()\n",
    "def completion_callback(input_filenames, user_data, infer_ctx, request_id):\n",
    "    user_data._completed_requests.put((request_id, input_filenames))\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "def model_dtype_to_np(model_dtype):\n",
    "    if model_dtype == model_config.TYPE_BOOL:\n",
    "        return np.bool\n",
    "    elif model_dtype == model_config.TYPE_INT8:\n",
    "        return np.int8\n",
    "    elif model_dtype == model_config.TYPE_INT16:\n",
    "        return np.int16\n",
    "    elif model_dtype == model_config.TYPE_INT32:\n",
    "        return np.int32\n",
    "    elif model_dtype == model_config.TYPE_INT64:\n",
    "        return np.int64\n",
    "    elif model_dtype == model_config.TYPE_UINT8:\n",
    "        return np.uint8\n",
    "    elif model_dtype == model_config.TYPE_UINT16:\n",
    "        return np.uint16\n",
    "    elif model_dtype == model_config.TYPE_FP16:\n",
    "        return np.float16\n",
    "    elif model_dtype == model_config.TYPE_FP32:\n",
    "        return np.float32\n",
    "    elif model_dtype == model_config.TYPE_FP64:\n",
    "        return np.float64\n",
    "    elif model_dtype == model_config.TYPE_STRING:\n",
    "        return np.dtype(object)\n",
    "    return None\n",
    "\n",
    "def parse_model(url, protocol, model_name, batch_size, verbose=False):\n",
    "    \"\"\"\n",
    "    Check the configuration of a model to make sure it meets the\n",
    "    requirements for an image classification network (as expected by\n",
    "    this client)\n",
    "    \"\"\"\n",
    "    ctx = ServerStatusContext(url, protocol, model_name, verbose)\n",
    "    server_status = ctx.get_server_status()\n",
    "\n",
    "    if model_name not in server_status.model_status:\n",
    "        raise Exception(\"unable to get status for '\" + model_name + \"'\")\n",
    "\n",
    "    status = server_status.model_status[model_name]\n",
    "    config = status.config\n",
    "\n",
    "    if len(config.input) != 2:\n",
    "        raise Exception(\"expecting 2 input, got {}\".format(len(config.input)))\n",
    "    if len(config.output) != 1:\n",
    "        raise Exception(\"expecting 1 output, got {}\".format(len(config.output)))\n",
    "\n",
    "    input_0 = config.input[0]\n",
    "    input_1 = config.input[1]\n",
    "\n",
    "    output = config.output[0]\n",
    "\n",
    "    if output.data_type != model_config.TYPE_FP32:\n",
    "        raise Exception(\"expecting output datatype to be TYPE_FP32, model '\" +\n",
    "                        model_name + \"' output type is \" +\n",
    "                        model_config.DataType.Name(output.data_type))\n",
    "\n",
    "    # Output is expected to be a vector. But allow any number of\n",
    "    # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10\n",
    "    # }, { 10, 1, 1 } are all ok). Variable-size dimensions are not\n",
    "    # currently supported.\n",
    "    non_one_cnt = 0\n",
    "    for dim in output.dims:\n",
    "        if dim == -1:\n",
    "            raise Exception(\"variable-size dimension in model output not supported\")\n",
    "        if dim > 1:\n",
    "            non_one_cnt += 1\n",
    "            if non_one_cnt > 1:\n",
    "                raise Exception(\"expecting model output to be a vector\")\n",
    "\n",
    "    # Model specifying maximum batch size of 0 indicates that batching\n",
    "    # is not supported and so the input tensors do not expect an \"N\"\n",
    "    # dimension (and 'batch_size' should be 1 so that only a single\n",
    "    # image instance is inferred at a time).\n",
    "    max_batch_size = config.max_batch_size\n",
    "    if max_batch_size == 0:\n",
    "        if batch_size != 1:\n",
    "            raise Exception(\"batching not supported for model '\" + model_name + \"'\")\n",
    "    else: # max_batch_size > 0\n",
    "        if batch_size > max_batch_size:\n",
    "            raise Exception(\"expecting batch size <= {} for model {}\".format(max_batch_size, model_name))\n",
    "\n",
    "    # Model input must have 3 dims, either CHW or HWC\n",
    "    if len(input_0.dims) != 3:\n",
    "        raise Exception(\n",
    "            \"expecting input to have 3 dimensions, model '{}' input has {}\".format(\n",
    "                model_name, len(input_0.dims)))\n",
    "\n",
    "    # Variable-size dimensions are not currently supported.\n",
    "    for dim in input_0.dims:\n",
    "        if dim == -1:\n",
    "            raise Exception(\"variable-size dimension in model input not supported\")\n",
    "\n",
    "    if ((input_0.format != model_config.ModelInput.FORMAT_NCHW) and\n",
    "        (input_0.format != model_config.ModelInput.FORMAT_NHWC)):\n",
    "        raise Exception(\"unexpected input format \" + model_config.ModelInput.Format.Name(input_0.format) +\n",
    "                        \", expecting \" +\n",
    "                        model_config.ModelInput.Format.Name(model_config.ModelInput.FORMAT_NCHW) +\n",
    "                        \" or \" +\n",
    "                        model_config.ModelInput.Format.Name(model_config.ModelInput.FORMAT_NHWC))\n",
    "\n",
    "    if input_0.format == model_config.ModelInput.FORMAT_NHWC:\n",
    "        h = input_0.dims[0]\n",
    "        w = input_0.dims[1]\n",
    "        c = input_0.dims[2]\n",
    "    else:\n",
    "        c = input_0.dims[0]\n",
    "        h = input_0.dims[1]\n",
    "        w = input_0.dims[2]\n",
    "\n",
    "    return (input_0.name, input_1.name, output.name, c, h, w, input_0.format, model_dtype_to_np(input_0.data_type))\n",
    "\n",
    "def preprocess(img, format, dtype, c, h, w, scaling):\n",
    "    \"\"\"\n",
    "    Pre-process an image to meet the size, type and format\n",
    "    requirements specified by the parameters.\n",
    "    \"\"\"\n",
    "    #np.set_printoptions(threshold='nan')\n",
    "\n",
    "    if c == 1:\n",
    "        sample_img = img.convert('L')\n",
    "    else:\n",
    "        sample_img = img.convert('RGB')\n",
    "\n",
    "    resized_img = sample_img.resize((w, h), Image.BILINEAR)\n",
    "    resized = np.array(resized_img)\n",
    "    if resized.ndim == 2:\n",
    "        resized = resized[:,:,np.newaxis]\n",
    "\n",
    "    typed = resized.astype(dtype)\n",
    "\n",
    "    if scaling == 'INCEPTION':\n",
    "        scaled = (typed / 128) - 1\n",
    "    elif scaling == 'VGG':\n",
    "        if c == 1:\n",
    "            scaled = typed - np.asarray((128,), dtype=dtype)\n",
    "        else:\n",
    "            scaled = typed - np.asarray((123, 117, 104), dtype=dtype)\n",
    "    else:\n",
    "        scaled = typed\n",
    "\n",
    "    # Swap to CHW if necessary\n",
    "    if format == model_config.ModelInput.FORMAT_NCHW:\n",
    "        ordered = np.transpose(scaled, (2, 0, 1))\n",
    "    else:\n",
    "        ordered = scaled\n",
    "\n",
    "    # Channels are in RGB order. Currently model configuration data\n",
    "    # doesn't provide any information as to other channel orderings\n",
    "    # (like BGR) so we just assume RGB.\n",
    "    return ordered\n",
    "\n",
    "def postprocess(results, filenames, batch_size):\n",
    "    \"\"\"\n",
    "    Post-process results to show classifications.\n",
    "    \"\"\"\n",
    "    if len(results) != 1:\n",
    "        raise Exception(\"expected 1 result, got {}\".format(len(results)))\n",
    "\n",
    "    batched_result = list(results.values())[0]\n",
    "    if len(batched_result) != batch_size:\n",
    "        raise Exception(\"expected {} results, got {}\".format(batch_size, len(batched_result)))\n",
    "    if len(filenames) != batch_size:\n",
    "        raise Exception(\"expected {} filenames, got {}\".format(batch_size, len(filenames)))\n",
    "\n",
    "    for (index, result) in enumerate(batched_result):\n",
    "        print(\"Image '{}':\".format(filenames[index]))\n",
    "        for cls in result:\n",
    "            print(\"    {} ({}) = {}\".format(cls[0], cls[2], cls[1]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-v', '--verbose', action=\"store_true\", required=False, default=False,\n",
    "                        help='Enable verbose output')\n",
    "    parser.add_argument('-a', '--async', dest=\"async_set\", action=\"store_true\", required=False,\n",
    "                        default=False, help='Use asynchronous inference API')\n",
    "    parser.add_argument('--streaming', action=\"store_true\", required=False, default=False,\n",
    "                        help='Use streaming inference API. ' +\n",
    "                        'The flag is only available with gRPC protocol.')\n",
    "    parser.add_argument('-m', '--model-name', type=str, required=True,\n",
    "                        help='Name of model')\n",
    "    parser.add_argument('-x', '--model-version', type=int, required=False,\n",
    "                        help='Version of model. Default is to use latest version.')\n",
    "    parser.add_argument('-b', '--batch-size', type=int, required=False, default=1,\n",
    "                        help='Batch size. Default is 1.')\n",
    "    parser.add_argument('-c', '--classes', type=int, required=False, default=1,\n",
    "                        help='Number of class results to report. Default is 1.')\n",
    "    parser.add_argument('-s', '--scaling', type=str, choices=['NONE', 'INCEPTION', 'VGG'],\n",
    "                        required=False, default='NONE',\n",
    "                        help='Type of scaling to apply to image pixels. Default is NONE.')\n",
    "    parser.add_argument('-u', '--url', type=str, required=False, default='localhost:8000',\n",
    "                        help='Inference server URL. Default is localhost:8000.')\n",
    "    parser.add_argument('-i', '--protocol', type=str, required=False, default='HTTP',\n",
    "                        help='Protocol (HTTP/gRPC) used to ' +\n",
    "                        'communicate with inference service. Default is HTTP.')\n",
    "    parser.add_argument('image_filename', type=str, nargs='?', default=None,\n",
    "                        help='Input image / Input folder.')\n",
    "    FLAGS = parser.parse_args()\n",
    "\n",
    "    protocol = ProtocolType.from_str(FLAGS.protocol)\n",
    "\n",
    "    if FLAGS.streaming and protocol != ProtocolType.GRPC:\n",
    "        raise Exception(\"Streaming is only allowed with gRPC protocol\")\n",
    "\n",
    "    # Make sure the model matches our requirements, and get some\n",
    "    # properties of the model that we need for preprocessing\n",
    "    input_0_name, input_1_name, output_name, c, h, w, format, dtype = parse_model(\n",
    "        FLAGS.url, protocol, FLAGS.model_name,\n",
    "        FLAGS.batch_size, FLAGS.verbose)\n",
    "\n",
    "    ctx = InferContext(FLAGS.url, protocol, FLAGS.model_name,\n",
    "                       FLAGS.model_version, FLAGS.verbose, 0, FLAGS.streaming)\n",
    "\n",
    "    filenames = []\n",
    "    if os.path.isdir(FLAGS.image_filename):\n",
    "        filenames = [os.path.join(FLAGS.image_filename, f)\n",
    "                     for f in os.listdir(FLAGS.image_filename)\n",
    "                     if os.path.isfile(os.path.join(FLAGS.image_filename, f))]\n",
    "    else:\n",
    "        filenames = [FLAGS.image_filename,]\n",
    "\n",
    "    filenames.sort()\n",
    "\n",
    "    # Preprocess the images into input data according to model\n",
    "    # requirements\n",
    "    image_data = []\n",
    "    for filename in filenames:\n",
    "        img = Image.open(filename)\n",
    "        image_data.append(preprocess(img, format, dtype, c, h, w, FLAGS.scaling))\n",
    "\n",
    "    # Send requests of FLAGS.batch_size images. If the number of\n",
    "    # images isn't an exact multiple of FLAGS.batch_size then just\n",
    "    # start over with the first images until the batch is filled.\n",
    "    results = []\n",
    "    result_filenames = []\n",
    "    request_ids = []\n",
    "    image_idx = 0\n",
    "    last_request = False\n",
    "    user_data = UserData()\n",
    "    sent_count = 0\n",
    "    while not last_request:\n",
    "        input_filenames = []\n",
    "        input_batch = []\n",
    "        for idx in range(FLAGS.batch_size):\n",
    "            input_filenames.append(filenames[image_idx])\n",
    "            input_batch.append(image_data[image_idx])\n",
    "            image_idx = (image_idx + 1) % len(image_data)\n",
    "            if image_idx == 0:\n",
    "                last_request = True\n",
    "\n",
    "        # Send request\n",
    "        if not FLAGS.async_set:\n",
    "            results.append(ctx.run(\n",
    "                { input_0_name : input_batch, input_1_name: input_batch },\n",
    "                { output_name : (InferContext.ResultFormat.CLASS, FLAGS.classes) },\n",
    "                FLAGS.batch_size))\n",
    "            result_filenames.append(input_filenames)\n",
    "        else:\n",
    "            ctx.async_run(partial(completion_callback, input_filenames, user_data),\n",
    "                            { input_0_name : input_batch, input_1_name: input_batch },\n",
    "                            { output_name : (InferContext.ResultFormat.CLASS, FLAGS.classes) },\n",
    "                            FLAGS.batch_size)\n",
    "            sent_count += 1\n",
    "\n",
    "    # For async, retrieve results according to the send order\n",
    "    if FLAGS.async_set:\n",
    "        processed_count = 0\n",
    "        while processed_count < sent_count:\n",
    "            (request_id, input_filenames) = user_data._completed_requests.get()\n",
    "            results.append(ctx.get_async_run_results(request_id))\n",
    "            result_filenames.append(input_filenames)\n",
    "            processed_count += 1\n",
    "\n",
    "    for idx in range(len(results)):\n",
    "        print(\"Request {}, batch size {}\".format(idx, FLAGS.batch_size))\n",
    "        postprocess(results[idx], result_filenames[idx], FLAGS.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_model'></a>\n",
    "### 2. Set up a model repository "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download the pre-trained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd triton-inference-server\n",
    "cd docs/examples\n",
    "./fetch_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implement an ensemble model and export the model to onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble model is model averaging ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ensemble_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleModel,self).__init__()\n",
    "        self.act1 = nn.Softmax(dim=1)\n",
    "    def forward(self,x1,x2): # x1 : FC x2: softmax\n",
    "        \n",
    "        return (self.act1(x1) + x2) / 2.0\n",
    "\n",
    "def onnx_export(model, x1 ,x2, onnx_path = 'model.onnx'):\n",
    "    torch.onnx.export(model,\n",
    "                        (x1,x2),\n",
    "                        onnx_path,\n",
    "                        opset_version=10,       \n",
    "                        do_constant_folding=True,\n",
    "                        input_names = ['data_0', 'data_1'],\n",
    "                        output_names = ['output'])\n",
    "if __name__ == \"__main__\":\n",
    "    model = EnsembleModel().cuda()\n",
    "    model.eval()\n",
    "    batch = 1\n",
    "    x1 = torch.randn(batch,1000, 1, 1, device= torch.device('cuda'))\n",
    "    x2 = torch.randn(batch,1000, 1, 1, device= torch.device('cuda'))\n",
    "    out = model(x1,x2)\n",
    "    onnx_export(model,x1,x2)\n",
    "    print(\"export onnx file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --name pytorch --rm --runtime=nvidia  -v $(pwd):/workspace nvcr.io/nvidia/pytorch:20.03-py3 python ensemble_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_ensemble'></a>\n",
    "### 3. Set up the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-inference-server/docs/examples/model_repository/PostModel_onnx/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv model.onnx triton-inference-server/docs/examples/model_repository/PostModel_onnx/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls triton-inference-server/docs/examples/model_repository/PostModel_onnx/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp triton-inference-server/docs/examples/model_repository/resnet50_netdef/resnet50_labels.txt triton-inference-server/docs/examples/model_repository/PostModel_onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls triton-inference-server/docs/examples/model_repository/PostModel_onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-inference-server/docs/examples/model_repository/PostModel_onnx/config.pbtxt\n",
    "name: \"PostModel_onnx\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"data_0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
    "  },\n",
    "  {\n",
    "    name: \"data_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
    "  }\n",
    "  \n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000]\n",
    "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
    "    label_filename: \"resnet50_labels.txt\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_scheduler'></a>\n",
    "### 4. Set up ensemble scheduler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/Ensemble_scheduler.png\" width=\"300\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-inference-server/docs/examples/model_repository/Ensemble_model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-inference-server/docs/examples/model_repository/Ensemble_model/config.pbtxt\n",
    "name: \"Ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"densenet_onnx\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"data_0\"\n",
    "        value: \"INPUT0\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"fc6_1\"\n",
    "        value: \"dense_out\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"resnet50_netdef\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"gpu_0/data\"\n",
    "        value: \"INPUT1\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"gpu_0/softmax\"\n",
    "        value: \"resnet_out\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"PostModel_onnx\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"data_0\"\n",
    "        value: \"dense_out\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"data_1\"\n",
    "        value: \"resnet_out\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"output\"\n",
    "        value: \"OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run_server'></a>\n",
    "### 5. Run triton inference serever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-docker run -d --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/triton-inference-server/docs/examples/model_repository/:/models nvcr.io/nvidia/tritonserver:20.03-py3  trtserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait while model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:8000/api/status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='request'></a>\n",
    "### 6. Request image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm -t --net=host -v $(pwd):/workspace/client --name client tritonserver_client python client/ensemble_image_client.py -m Ensemble_model -s INCEPTION images/mug.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
